---
title: "Ex-02.2-ghq"
subtitle: ""
author:
 - name: Paul Johnson
   affiliation: CRMDA
   email: pauljohn@ku.edu
abstract: >
    RHS anorexia exercise
checked_by: "First Last"
Note to Authors: please_dont_change_the_next 4 lines!
date: "`r format(Sys.time(), '%b. %e %Y')`"
output:
  crmda::crmda_html_document:
    toc: true
    toc_depth: 2
    highlight: haddock
logoleft: /home/pauljohn/R/x86_64-pc-linux-gnu-library/3.4/crmda/theme/jayhawk.png
logoright: /home/pauljohn/R/x86_64-pc-linux-gnu-library/3.4/crmda/theme/CRMDAlogo-vert.png
---

# Data import
```{r import}
if (!file.exists("ghq.csv")){
    ghq <- data.frame(
        id = seq(1, 12, by =1),
        g = c(12, 8, 22, 10, 10, 6, 8, 4, 14, 6, 2, 22, 12, 7, 24, 14, 8, 4, 5, 6, 14, 5, 5, 16),
        occasion = rep(c(1,2), each = 12)
    )
    write.csv(ghq, "ghq.csv", row.names = FALSE)
} else {
    ghq <- read.csv("ghq.csv")
}
```

```{r recode10}
rockchalk::summarize(ghq)
ghq$id.orig <- ghq$id
ghq$id <- as.factor(ghq$id)
```

# 2.2.1: No Reshape Required

We created the ghq data in the long format, so no reshaping is needed.

```{r recode20}
ghq
```

# 2.2.1b: mixed model with REML


```{r lmer10}
library(lme4)
m1 <- lmer(g ~ (1|id), data = ghq, REML = TRUE)
summary(m1)
```

<!-- ```{r lmer20} -->
<!-- library(lmerTest) -->
<!-- m1b <- lmer(wp ~ (1|id), data = pefrlong, REML = FALSE) -->
<!-- summary(m1b) -->
<!-- ``` -->

```{r confint}
confint(m1, method = "profile")
```


# 2.2.1c: predictions of random effects

## Retrieve predictions of the random effects from m1.


```{r blups10}
m1.ranef <- ranef(m1, condVar = TRUE)
m1.ranef
```

```{r dotplot10}
library(lattice)
dotplot(m1.ranef)
```

This is a way of "extracting" the vector of posterior modes of the
group deviations about the overall center point.

```{r}
m1.ranef.byid <- m1.ranef[["id"]][ , "(Intercept)"]
m1.ranef.byid
```

The vector `m1.ranef.byid` is generated by lme4 using penalized least
squares estimation.  This means that it is not, actually, calculated
by the formula we find in the textbooks for posterior modes of random
effects.  However, the usual formula can be used to produce
the same thing, in (almost all) cases I've tried. 

Here is the "usual formula", with words in place of some symbols:

\[
EmpBayes~Ranef = \lambda_j group~average_j + (1-\lambda_j) overall~average
\]

This formula is simple. The term I call `overall average` is the mean
of observed scores. It is the intercept we get when we run a
regression with no predictors. It is same as we would get taking an
average of all scores, of course.

```{r}
lm0 <- lm(g ~ 1, data = ghq)
overallavg <- unname(coef(lm0)[1])
overallavg
```

On the other hand, the average within each group is the "fixed
effects" estimate obtained by a linear model that does not include an
intercept.

```{r}
lm1 <- lm(g ~ 0 + id, data = ghq)
groupavg <- coef(lm1)
groupavg
```

In R, we can get the same result by using the `by` or `aggregate`
functions, like so

```{r}
aggregate(ghq$g, list(id = ghq$id), mean, na.rm = TRUE)
```

To reproduce the "empirical bayes" statements about group averages, we
now need to calculate $\lambda_j$. The formula depends on the
estimated variances of $\varepsilon$ (individual row level random
effect) the variance of the group-level intercept.

\[
\lambda_j = \frac{var~of~random~effect}{var~of~random~effect +
\frac{var~of~residual}{n~of~cases~group~j}}
\]

Because both of our groups in this case have 2 observations for each
of 12 groups, this is very easy to calculate. The $\lambda_j$ is
actually the same for all groups. If you allow me to simply read the
output from the regression output above, I'll take the standard
deviations of the random effect and square them, as:

```{r}
lambda <- 5.92^2 / (5.92^2 + (1.915^2)/rep(2, 12))
lambda
```

```{r}
eb.blups <- lambda * groupavg + (1 - lambda) * overallavg
eb.blups
```

Note that the empirical Bayes statements about the group averages
are the same values returned by the `coef` function in `lme4`:

```{r}
coef(m1)
```

A student wonders why are these not the same as the values reported
by `ranef(m1)` from `lme4`, and the answer is that the random effects
are variations that fluctuate around the overall average.  Hence, the
predictions of the group averages are equal to the random effects
\emph{plus} the average, as we see here:

```{r}
overallavg + m1.ranef.byid
```

When I did those calculations, I gave myself an easy approach by
reading the estimates 5.92 and 1.915 from the model output and
typing those in.  It turns out it is not too easy to extract those
values from the `lme4` fitted model, but here is how it can be done.

The estimated standard deviation of the residual error term, which
should be 1.915 in the output (rounded).

```{r}
m1.resid.sd <- attr(VarCorr(m1), "sc")
m1.resid.sd
## Again, not easy. That is tucked away as an attribute of VarCorr output
```

The the estimated standard deviation of the random effect, the group
intercept term, is 5.92 in the output and we can extract it from the model:

```{r}
m1.id.sd <- attr(VarCorr(m1)[["id"]], "stddev")
m1.id.sd
```

And the number within each group can be retrieved as

```{r}
npergroup <- aggregate(ghq$id, list(id = ghq$id), length)
```

but since that is 2 for each group, it hardly seems worth the effort.


```{r}
## shrinkage <- psi_hat/(psi_hat + (theta_hat/12))
lambda2 <- m1.id.sd^2 / (m1.id.sd^2 + (m1.resid.sd^2 / npergroup$x))
lambda2
```

The values in `lambda2` are more accurate because I did not type in
the values, I retrieved them in full numerical precision from the
model. But they are, practically speaking, equivalent. See?

```{r}
lambda
```

The difference is trivial:

```{r}
lambda2 - lambda
```

# 2.2.3 Allow occasion as a predictor

```{r}
m2 <- lmer(g ~ factor(occasion)  + (1|id), data = ghq)
summary(m2)
```

The fact that the t-statistic for the occassion is not
statistically significant (t = -0.41, p = 0.68) indicates
that there is no "mean shift" between measurement occasions.
